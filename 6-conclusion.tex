\chapter{Conclusion}
\label{chap:conclusion}

In this chapter, we summarise the main contributions of this thesis, discuss its implications and suggest future work directions.

\section{Summary}

In this thesis, we introduced KryBall, a novel second-order optimisation algorithm designed to address fundamental limitations in current optimisation methods for deep learning. We make a number of contributions that span both the practical and theoretical aspects of optimisation. 

For our theoretical contributions, we present the KryBall algorithm that combines the Saddle-Free-Newton method with a Krylov-subspace approach and integrates this within a trust-region framework. We connect the mathematical foundations of Krylov methods to practical optimisation performance, and address the issue of saddle point proliferation in high-dimensional spaces. We provide systematic analysis of loss landscape behaviour, problem conditioning, the effect of activation functions, and identify cases where our method excels and fails (such as when non-smooth activations are used). We also provide analytical tools for assessing the effectiveness of Krylov subspace approximations and demonstrate how to quantify the approximation error in our approach.

For the practical contributions, we present an implementation of Krylov-based Saddle-Free Newton methods integrated with a trust-region approach. We present the first $N$-dimensional subspace optimiser fully integrated with PyTorch, alongside an extensible testing suite. We also present our evaluations on ill-conditioned functions and classification tasks, where we demonstrate that KryBall is competitive with state-of-the-art methods, achieves rapid convergence on specific tasks, and is particularly well-suited to ill-conditioned problems.

\section{Future Work}
\label{sec:future_work}

While KryBall demonstrates promising performance across several optimisation tasks, our evaluation reveals several avenues for improvement and extension. The limitations identified in \cref{sec:limitations}, particularly around late-epoch instability, scalability, and dependence on smooth activation functions, point to promising research directions that can enhance our method. In this section, we outline key areas where future research could build upon our work to develop more effective and broadly applicable second-order optimisation methods. We discuss hybrid approaches, the need for theoretical rigour, improved model evaluations, support for non-smooth optimisation and computational efficiency.

\subsection{Hybrid Approaches}
\label{ssec:hybrid_approaches}

Our analysis in \cref{ssec:late_epoch_instability} revealed that KryBall exhibits late-epoch instability when the loss landscape transitions from quadratic to subquadratic behavior near convergence. A promising direction is developing hybrid optimisation schemes that dynamically switch between second-order and first-order methods based on local landscape. There are many approaches to do this. For example, we could run KryBall for the first $k$ iterations until we are no longer confident that our quadratic approximation is accurate, and then switch to MSGD or Adam. We could also run KryBall every $k$ iterations, interweaving the updates with MSGD or Adam through training. The second approach has already seen wide use. The Sophia optimiser, which we covered in \cref{chap:lit_review}, is an example of this approach, as it weaves its second order update with signSGD \citep{liu2023sophia}.

\subsection{Theoretical Rigour}
\label{sec:theoretical_rigour}

While we empirically evaluate KryBall and can adhere to the rapid convergence it exhibits in tasks such as ill-conditioned function optimisation and XOR classification, we currently lack strong theoretical guarantees. Future research should focus on establishing formal convergence rates and characterising the conditions in which approximation methods hold. For example, this involves developing theoretical frameworks that connect the reconstruction error analysis to actual convergence properties. We can further extend this by establishing error bounds on the approximation methods we use. This includes looking at the theoretical worst and best case scenarios, as well as introducing an expectation on the error. 

\subsection{Improved Model Evaluations}
\label{ssec:improved_model_evaluations}

Our evaluation focused primarily on relatively small-scale problems, with ResNet-18 on CIFAR-10 representing the largest model tested. Future work should evaluate KryBall's scalability to contemporary large-scale applications such as transformer models, large convolutional networks, and other modern architectures. We also recommend expanding our evaluation suite to include more diverse tasks rather than just image classification. Examples of such tasks include natural language processing, such as language modelling on the Penn Treebank dataset \citep{penntreebank}, which involves predicting the next word in a sentence. Other tasks include 2D Neural Radiance Fields \citep{mildenhall2021nerf}. This involves predicting the radiance of a scene from a single image, and activation maximisation, which involves optimising an input image to maximise the activation of a specific neuron in a neural network \citep{activation_maximisation}.

\subsection{Non-Smooth Optimisation}
\label{ssec:non_smooth_optimisation_last}

Our analysis demonstrated that KryBall's performance degrades significantly with non-smooth activation functions like ReLU due to violations of the quadratic model's smoothness assumptions. Future research should explore specialised variants that handle non-differentiable points more effectively. This could be done by using finite difference approximations, as was discussed in \cref{sec:non_smooth_optimisation} or developing smoothed approximations of non-smooth activations, Another direction involves investigating how recent advances in non-smooth optimiation theory could be incorporated into Krylov-based methods to maintain second-order benefits while handling piecewise linear functions.

\subsection{Computational Efficiency}
\label{ssec:computational_efficiency}
We discussed the limitation of having computational overhead in \cref{comp_overhead}. As such, future work could focus on exploring ways to reduce the computation overhead associated with second-order methods. First, developing more efficient Arnoldi iteration implementations that exploit sparsity patterns or low-rank structure could reduce the cost of subspace construction. Secondly, we can investigate adaptive schemes that dynamically adjust hyperparameters depending on the current point in the optimisation landscape. This ensures that parameters like the Krylov dimension $M$ and the Krylov refresh rate $r_\textit{refresh}$ are only updated when necessary. Thirdly, we explore research into creating parallelisable and distributed computing algorithms for common matrix operations, such as matrix inversion and eigendecomposition. These directions could significantly improve practical performance, and in combination with researching theoretical guarantees as stated in \cref{sec:theoretical_rigour}, we could ensure that second-order methods are more widely adaptable.

\section{Concluding Thoughts}
\label{sec:concluding_thoughts}

Optimisation in deep learning is a challenging problem, but it is fundamental for advancing the state-of-the-art. Optimisation has led to the development of powerful models, techniques and novel innovations. The work presented in this thesis is a step towards more efficient optimisation. We present KryBall, our second-order optimisation algorithm that combines the advantages of first and second-order methods by using Krylov subspaces, the Saddle-Free-Newton, and a trust-region framework. We thus take a step towards the goal of efficiently navigating the optimisation landscape, and improving the performance of integral machine learning models and tasks in today's world.
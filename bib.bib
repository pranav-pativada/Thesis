@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})
@String(ICML = {Int. Conf. Mach. Learn.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})

@article{imagenet,
  title={ImageNet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  journal={Communications of the ACM},
  volume={60},
  number={6},
  pages={84--90},
  year={2017},
  publisher={AcM New York, NY, USA}
}

@article{alphafold,
  title={Highly accurate protein structure prediction with AlphaFold},
  author={Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\v{Z}}{\'\i}dek, Augustin and Potapenko, Anna and others},
  journal={nature},
  volume={596},
  number={7873},
  pages={583--589},
  year={2021},
  publisher={Nature Publishing Group}
}


@article{alspector1992parallel,
  title   = {A parallel gradient descent method for learning in analog VLSI neural networks},
  author  = {Alspector, Joshua and Meir, Ronny and Yuhas, B and Jayakumar, Anthony and Lippe, D},
  journal = {NIPS},
  volume  = {5},
  year    = {1992}
}

@article{chen2024symbolic,
  title   = {Symbolic discovery of optimization algorithms},
  author  = {Chen, Xiangning and Liang, Chen and Huang, Da and Real, Esteban and Wang, Kaiyuan and Pham, Hieu and Dong, Xuanyi and Luong, Thang and Hsieh, Cho-Jui and Lu, Yifeng and others},
  journal = {NIPS},
  volume  = {36},
  year    = {2024}
}

@inproceedings{choromanska2015loss,
  title        = {The loss surfaces of multilayer networks},
  author       = {Choromanska, Anna and Henaff, Mikael and Mathieu, Michael and Arous, G{\'e}rard Ben and LeCun, Yann},
  booktitle    = {Artificial intelligence and statistics},
  pages        = {192--204},
  year         = {2015},
  organization = {PMLR}
}

@book{conn2009derivfree,
  title     = {Introduction to derivative-free optimization},
  author    = {Conn, Andrew R and Scheinberg, Katya and Vicente, Luis N},
  year      = {2009},
  publisher = {SIAM}
}

@article{dauphin2014sfn,
  title   = {Identifying and attacking the saddle point problem in high-dimensional non-convex optimization},
  author  = {Dauphin, Yann N and Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},
  journal = {NIPS},
  volume  = {27},
  year    = {2014}
}

@article{dozat2016nadam,
  title  = {Incorporating nesterov momentum into adam},
  author = {Dozat, Timothy},
  year   = {2016}
}

@article{duchi2011adagrad,
  title   = {Adaptive subgradient methods for online learning and stochastic optimization.},
  author  = {Duchi, John and Hazan, Elad and Singer, Yoram},
  journal = {Journal of machine learning research},
  volume  = {12},
  number  = {7},
  year    = {2011}
}

@inproceedings{henriques2019small,
  title     = {Small steps and giant leaps: Minimal newton solvers for deep learning},
  author    = {Henriques, Jo{\~a}o F and Ehrhardt, Sebastien and Albanie, Samuel and Vedaldi, Andrea},
  booktitle = {ICCV},
  pages     = {4763--4772},
  year      = {2019}
}

@article{johnson2013accelerating,
  title   = {Accelerating stochastic gradient descent using predictive variance reduction},
  author  = {Johnson, Rie and Zhang, Tong},
  journal = {NIPS},
  volume  = {26},
  year    = {2013}
}

@article{kingma2014adam,
  title   = {Adam: A method for stochastic optimization},
  author  = {Kingma, Diederik P},
  journal = {arXiv preprint arXiv:1412.6980},
  year    = {2014}
}

@article{LBFGS,
  issn      = {00255718, 10886842},
  url       = {http://www.jstor.org/stable/2006193},
  abstract  = {We study how to use the BFGS quasi-Newton matrices to precondition minimization methods for problems where the storage is critical. We give an update formula which generates matrices using information from the last $m$ iterations, where $m$ is any number supplied by the user. The quasi-Newton matrix is updated at every iteration by dropping the oldest information and replacing it by the newest information. It is shown that the matrices generated have some desirable properties. The resulting algorithms are tested numerically and compared with several well-known methods.},
  author    = {Jorge Nocedal},
  journal   = {Mathematics of Computation},
  number    = {151},
  pages     = {773--782},
  publisher = {American Mathematical Society},
  title     = {Updating Quasi-Newton Matrices with Limited Storage},
  urldate   = {2024-10-25},
  volume    = {35},
  year      = {1980}
}

@article{liu2023sophia,
  title   = {Sophia: A scalable stochastic second-order optimizer for language model pre-training},
  author  = {Liu, Hong and Li, Zhiyuan and Hall, David and Liang, Percy and Ma, Tengyu},
  journal = {arXiv preprint arXiv:2305.14342},
  year    = {2023}
}

@article{loshchilov2017decoupled,
  title   = {Decoupled weight decay regularization},
  author  = {Loshchilov, I},
  journal = {arXiv preprint arXiv:1711.05101},
  year    = {2017}
}

@inproceedings{martens2010hessianfree,
  title     = {Deep learning via hessian-free optimization.},
  author    = {Martens, James and others},
  booktitle = {ICML},
  volume    = {27},
  pages     = {735--742},
  year      = {2010}
}

@article{nemirovski2009robust,
  title     = {Robust stochastic approximation approach to stochastic programming},
  author    = {Nemirovski, Arkadi and Juditsky, Anatoli and Lan, Guanghui and Shapiro, Alexander},
  journal   = {SIAM Journal on optimization},
  volume    = {19},
  number    = {4},
  pages     = {1574--1609},
  year      = {2009},
  publisher = {SIAM}
}

@inproceedings{nesterov1983method,
  title     = {A method for unconstrained convex minimization problem with the rate of convergence O (1/k2)},
  author    = {Nesterov, Yurii},
  booktitle = {Dokl. Akad. Nauk. SSSR},
  volume    = {269},
  number    = {3},
  pages     = {543},
  year      = {1983}
}

@book{NoceWrig06,
  author    = {Jorge Nocedal and Stephen J. Wright},
  publisher = {Springer},
  title     = {Numerical Optimization},
  year      = {2006},
  address   = {New York, NY, USA},
  edition   = {2e}
}

@article{polyak1964some,
  title     = {Some methods of speeding up the convergence of iteration methods},
  author    = {Polyak, Boris T},
  journal   = {Ussr computational mathematics and mathematical physics},
  volume    = {4},
  number    = {5},
  pages     = {1--17},
  year      = {1964},
  publisher = {Elsevier}
}

@article{reddi2019asmgrad,
  title   = {On the convergence of adam and beyond},
  author  = {Reddi, Sashank J and Kale, Satyen and Kumar, Sanjiv},
  journal = {arXiv preprint arXiv:1904.09237},
  year    = {2019}
}

@misc{rmsprop2012,
  title  = {RMSProp, COURSERA: Neural Networks for Machine Learning},
  author = {Geoff, Hinton},
  year   = {2012}
}

@article{robbins1951stochastic,
  title     = {A stochastic approximation method},
  author    = {Robbins, Herbert and Monro, Sutton},
  journal   = {The annals of mathematical statistics},
  pages     = {400--407},
  year      = {1951},
  publisher = {JSTOR}
}

@article{ruder2016overview,
  title  = {An overview of gradient descent optimization algorithms},
  author = {Ruder, Sebastian},
  year   = {2016}
}

@article{sun2019survey,
  title     = {A survey of optimization methods from a machine learning perspective},
  author    = {Sun, Shiliang and Cao, Zehui and Zhu, Han and Zhao, Jing},
  journal   = {IEEE transactions on cybernetics},
  volume    = {50},
  number    = {8},
  pages     = {3668--3681},
  year      = {2019},
  publisher = {IEEE}
}

@inproceedings{sutskever2013importance,
  title        = {On the importance of initialization and momentum in deep learning},
  author       = {Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  booktitle    = {ICML},
  pages        = {1139--1147},
  year         = {2013},
  organization = {PMLR}
}

@article{taniguchi2024adopt,
  title   = {ADOPT: Modified Adam Can Converge with Any $\beta_2$ with the Optimal Rate},
  author  = {Taniguchi, Shohei and Harada, Keno and Minegishi, Gouki and Oshima, Yuta and Jeong, Seong Cheol and Nagahara, Go and Iiyama, Tomoshi and Suzuki, Masahiro and Iwasawa, Yusuke and Matsuo, Yutaka},
  journal = {NIPS},
  year    = {2024}
}

@inproceedings{vinyals2012krylov,
  title        = {Krylov subspace descent for deep learning},
  author       = {Vinyals, Oriol and Povey, Daniel},
  booktitle    = {Artificial intelligence and statistics},
  pages        = {1261--1268},
  year         = {2012},
  organization = {PMLR}
}

@inproceedings{yao2021adahessian,
  title     = {Adahessian: An adaptive second order optimizer for machine learning},
  author    = {Yao, Zhewei and Gholami, Amir and Shen, Sheng and Mustafa, Mustafa and Keutzer, Kurt and Mahoney, Michael},
  booktitle = {AAAI},
  volume    = {35},
  number    = {12},
  pages     = {10665--10673},
  year      = {2021}
}

@article{yousefi2023deep,
  title     = {Deep neural networks training by stochastic quasi-newton trust-region methods},
  author    = {Yousefi, Mahsa and Mart{\'\i}nez, {\'A}ngeles},
  journal   = {Algorithms},
  volume    = {16},
  number    = {10},
  pages     = {490},
  year      = {2023},
  publisher = {MDPI}
}

@article{zeiler2012adadelta,
  title   = {ADADELTA: an adaptive learning rate method},
  author  = {Zeiler, Matthew D},
  journal = {arXiv preprint arXiv:1212.5701},
  year    = {2012}
}

@article{greydanus_mnist1d,
  title={Scaling down deep learning with mnist-1d},
  author={Greydanus, Sam and Kobak, Dmitry},
  journal={arXiv preprint arXiv:2011.14439},
  booktitle={ICML},
  year={2020}
}

@article{cifar10,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Toronto, ON, Canada}
}

@inproceedings{resnet,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@article{ma2022beyond,
  title={Beyond the quadratic approximation: the multiscale structure of neural network loss landscapes},
  author={Ma, Chao and Kunin, Daniel and Wu, Lei and Ying, Lexing},
  journal={arXiv preprint arXiv:2204.11326},
  year={2022}
}

@article{tinysubspaces,
  title={Does SGD really happen in tiny subspaces?},
  author={Song, Minhak and Ahn, Kwangjun and Yun, Chulhee},
  journal={ICML},
  year={2025}
}

@inproceedings{finite_diff,
  title={Robustness via curvature regularization, and vice versa},
  author={Moosavi-Dezfooli, Seyed-Mohsen and Fawzi, Alhussein and Uesato, Jonathan and Frossard, Pascal},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={9078--9086},
  year={2019}
}

\chapter{Background}
\label{chap:background}

In this chapter, we provide the relevant technical background required to understand our work. We start by introducing the optimisation problem and the relevant concepts in \cref{sec:optimisation_problem}, continued by a discussion of optimisation in deep learning and the prevalence of saddle points. We then discuss numerical optimisation methods in the form of Krylov subspaces and trust regions.

\section{The Optimisation Problem}
\label{sec:optimisation_problem}

In this section, we formalise the optimisation problem. In the simplest case, we minimise an objective function $f$ with respect to real-valued variables. The formulation is 
\begin{align}
    \min_{x} f(x)
\end{align}
where $x \in \mathbb{R}^n$ given $n \geq 1$ and $f: \mathbb{R}^n \to \mathbb{R}$.


\subsection{The Geometry of Optimisation}
\label{sec:geometry_of_optimisation}

\subsection{Unconstrained vs Constrained Optimisation}
So far, we have looked primarily at unconstrained optimisation. 
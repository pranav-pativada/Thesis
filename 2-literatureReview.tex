\chapter{Related Works}
\label{chap:lit_review}

% Categorisation/grouping of the literature
% For each cluster, have a section, covers the major academic works thoroughly
%% Describe, advantages/disadvantages including assumptions, major findings, some analysis
%% Compare to self
% Give the impression that you understand the field thoroughly

Optimisation methods for deep learning have gone through a variety of works that improve the ability to navigate through loss landscapes.
The work in this thesis is heavily inspired by such advances. In this chapter, we explore the major academic works that have played a significant role in the field of optimisation. We provide an overview of key optimisation methods in two main parts: first-order methods and higher-order methods. We also look at other techniques in the field, namely derivative-free optimisation and meta-learning.

\section{First-Order Methods}
In the previous chapter, we saw that the aim of an optimiser is to find $\theta^* = \arg\min_{\theta} F(\theta)$ where $F$ is our objective function to optimise and $\theta$ are the parameters of $F$. First-order methods are characterised under comptuing the first order derivative $\nabla_{\theta}F$ to use in optimisation. We refer to this as $g$, or otherwise $g_t$ if in a stochastic setting.

\subsection{Gradient-Based Methods}
\textbf{Gradient Descent}: One of the earliest methods deployed to solve such optimisation problems is gradient descent (GD), which iteratively updates the parameters with a scalar multiple of the negative gradient direction at each step, $\theta = \theta - \alpha g$. Here, the $\alpha$ parameter is the \textit{learning rate} which controls the step size of each iteration \citep{ruder2016overview}. GD gradually reaches the optimal value of $F$, and given appropriate $\alpha$, reaches the global optimum sub-linearly if $F$ is \textit{convex}, converging linearly if $F$ is \textit{strongly convex} \citep{NoceWrig06}. However, GD is expensive. In many machine learning tasks, $F$ is evaluated with $N$ samples, each with dimensionality $D$ - and so GD requires $O(ND)$ per iteration to evaluate. This becomes infeasible very quickly as $N$ and $D$ scale, and disallows online updates and limits model adaptability \citep{ruder2016overview}. While some parallelisation methods were proposed to combat this, the scalability issue still remains large at hand \citep{alspector1992parallel,NoceWrig06}.

\textbf{Stochastic Gradient Descent}: To address the limitations of GD, stochastic gradient descent (SGD) was proposed. Instead of evaluating $g$ using all $N$, a random sample $n \in N$ is picked and the stochastic gradient $g_t$, which is an unbiased estimate of the real gradient, is computed to update $\theta$ \citep{robbins1951stochastic}. SGD achieves the same convergence rates as GD given $F$ satisfies the same convexity conditions \citep{johnson2013accelerating, nemirovski2009robust}. Each iteration reduces from $O(ND)$ to $O(D)$ given only one sample is used.
This overcomes the disadvantage of GD in two ways: online updates can be performed, and convergence can be accelerated, as an optimal $\theta$ can be found using $n << N$ samples \citep{johnson2013accelerating, nemirovski2009robust}. However, the stochastic gradients used in SGD are inherently noisy and fluctuate. These fluctuations can be beneficial, as they allow SGD to jump from one place to another. This behavior is particularly useful in high-dimensional spaces, where most local minima are approximately equal to the global minimum as the number of parameters increases.

On the other hand, if SGD gets trapped in regions such as saddle points where it is hard to make progress, the fluctuations become less effective. These fluctuations can also lead to large variance in the gradient estimates \citep{sun2019survey}. This can make the optimization process unstable and slow convergence. Convergence is also affected by the learning rate, $\alpha$. Setting a too low $\alpha$ will slow convergence, and setting a too high $\alpha$ can hinder convergence all together, either overshooting or oscillating at the minimum. Choosing a good $\alpha$ usually requires manual tuning, which is arduous and compute heavy.

\textbf{Mini-batch Stochastic Gradient Descent}: The compromise between SGD and GD resulted in mini-batch stochastic gradient descent (mini-batch SGD) \citep{robbins1951stochastic}. mini-batch SGD splits the $N$ samples into $m << N$ i.i.d \textit{batches} (usually ranging from 64 to 256). $\theta$ is updated each iteration by with the stochastic batch gradient based off one batch. Over time, all batches are processed. This reduces the variance in the gradients and makes convergence more stable, which helps optimisation speed, though we note that the $\alpha$ parameter is still needed. mini-batch SGD is now a mainstream technique used for optimising machine learning models. To align with the standard in the literature, we will refer to mini-batch SGD as SGD from now on.

\subsection{Momentum Methods} \label{sec:momentum}
\textbf{Momentum SGD}: Many works have been proposed to improve upon SGD, one of which is momentum (MSGD) \citep{polyak1964some}. The momentum method augments SGD with a \textit{momentum} variable $z$, a decaying average of past gradients controlled by a \textit{momentum factor} $\beta$ \citep{polyak1964some, henriques2019small}. The update is then performed using $z$. 
\begin{align}
    z_{t+1} &= \beta z_{t} - g_t \\
    \theta_{t+1} &= \theta_t + \alpha z_{t+1}
\end{align}
This encourages progress along consistent, but small gradient directions. While this doubles the parameter space needed for updates (since we need $\alpha$ and $\beta$), MSGD yields faster convergence, remains more stable under changing $\alpha$, and is more resistant if $F$ scales poorly \citep{polyak1964some,henriques2019small}. However, this adds another parameter to tune, and the same problems as seen with $\alpha$ arise. Setting a $\beta$ too low won't result in improvements, and setting it too high can result in overshooting.

\textbf{Nesterov Accelerated SGD}: A small improvement over MSGD is Nesterov-Accelerated SGD (NAG). NAG first updates $\theta$ with $z$, and then calculates the gradient based on the updated $\theta$ \citep{nesterov1983method}.
\begin{align}
    \Tilde{\theta}_{t} &= \theta_{t} + \beta z_{t} \\
    z_{t+1} &= \beta z_t - \nabla_{\theta} F(\Tilde{\theta}_{t}) \\
    \theta_{t+1} &= \theta_{t} + \alpha z_{t+1}
\end{align}
Updating based on the future position of $\theta$ includes more gradient information in comparison to traditional momentum, which can provide a better direction \citep{nesterov1983method, dozat2016nadam}. NAG can also result in superlinear convergence when there is no stochasticity \citep{nesterov1983method}.

\subsection{Adaptive Methods}

\textbf{AdaGrad}: To tackle the problem of setting an appropriate $\alpha$, adaptive methods that adjust or scale $\alpha$ directly per parameter were proposed \cite{duchi2011adagrad}. The AdaGrad algorithm keeps a history of previous gradients and adjusts the learning rate dynamically \citep{duchi2011adagrad}. It accumulates the historical gradients via squaring, then scales the current gradient \citep{duchi2011adagrad}. Note that the $\epsilon$ term is added for numerical stability.
\begin{align}
    V_t &= \sqrt{\sum_{i=1}^{t} g_i^2} \\
    \theta_{t+1} &= \theta_{t} + \alpha \frac{g_t}{V_t + \epsilon}
\end{align}
This means only an initial $\alpha$ needs to be set, as during each update the learning rate will adapt due to scaling. Though, as $t \rightarrow \infty$, $V_{t} \rightarrow \infty$, and the learning rate is driven towards zero as the denominator in the update term becomes very large \citep{ruder2016overview,rmsprop2012,zeiler2012adadelta}.
This makes the parameter updates ineffective. This is especially the case in non-convex settings and regions such as saddle points, as the learning rate will be very quickly driven to zero before making sufficient progress.

\textbf{RMSProp and AdaDelta}: To address this, instead of accumulating all of the gradients, RMSProp takes inspiration from momentum \citep{ruder2016overview, rmsprop2012}. It uses an exponential decaying moving average to weight the past squared gradients using a decay parameter $p$ \citep{rmsprop2012, kingma2014adam}.
\begin{align}
    V_t &= \sqrt{p V_{t-1} + (1 - p)g_t^2} 
\end{align}
This simulates having a window size $w$ - where gradients outside of this window are forgotten \citep{rmsprop2012, kingma2014adam}. It priorities more recent gradients and so the learning rate is more stable and doesn't diminish with time. AdaDelta improves upon this and also keeps track of the change in updates, $\Delta(\theta_t)$, performed \citep{zeiler2012adadelta}.
\begin{align}
    U_t = \sqrt{p U_{t-1} + (1 - p)(\Delta(\theta_t))^2}
\end{align}
It uses these accumulative updates at each iteration to perform the actual update which is 
\begin{align}
    \theta_{t+1} = \theta_{t} - \frac{U_{t-1}}{V_{t} + \epsilon} g_t
\end{align}
The key observation here is the absence of $\alpha$. This is a step up from most SGD methods, which require the $\alpha$ parameter. The algorithm is also suitable in non-convex settings since there is no diminishing learning rate \citep{zeiler2012adadelta}. However, the absence of $\alpha$ is replaced with $p$ and $\epsilon$ which still need to be tuned, though these are much less sensitive.

\textbf{Adam}: Perhaps the most successful algorithm for optimisation is Adaptive Moment Estimation (Adam) \citep{kingma2014adam}. Adam maintains two moving averages: one for the gradients themselves, $m_t$, and the same $V_t$ used in RMSProp/AdaDelta \citep{kingma2014adam}. Each of these have an associated decay term $\beta_1$ and $\beta_2$, which weight the gradients and the history \citep{kingma2014adam}.
\begin{align}
m_t &= \beta_1 m_{t-1} + (1 - \beta_1) g_t \label{eq:m_t} \\
V_t &= \sqrt{\beta_2 V_{t-1} + (1 - \beta_2) g_t^2} \\
\hat{m_t} &= \frac{m_t}{1 - (\beta_1)^t} \\
\hat{V_t} &= \frac{V_t}{1 - (\beta_2)^t} \\
\theta_{t+1} &= \theta_t - \alpha \frac{\hat{m_t}}{{\hat{V_t}} + \epsilon} \label{eq:adam_update}
\end{align}
We call $m_t$ and $V_t$ the \textit{first} and \textit{second} moment estimates respectively, with $\beta_1$ and $\beta_2$ being the first and second moment hyper-parameters \citep{kingma2014adam}. Adam has a few key advantages. It uses momentum and exponential decay to smooth out gradients, which makes the optimisation process more stable. It also has bias correction applied to $\hat{m_t}$ and $\hat{V_t}$ to allow for proper initialisation \citep{kingma2014adam}. This ensures they are not biased towards their starting estimates. This is important during early stages of optimisation, as there is insufficient data to estimate the moments. Adam is stable and effective in practice, and works well with both convex and non-convex $F$ - and is the current SOTA optimisation method in deep learning. However, it lacks theoretical guarantees and convergence is not well understood \citep{reddi2019convergence}. Adam has been shown to fail in simple one dimensional convex settings \citep{reddi2019convergence}. It additionally needs two extra parameters, $\beta_1$ and $\beta_2$, in comparison to traditional SGD, which need to be tuned.

\subsection{Adam Variants}
\textbf{AdaMax}: Several variants of Adam have been proposed to address it's limitations. The first variant is AdaMax. AdaMax replaces the $L_2$ norm in $V_t$ with the $L_{\infty}$ norm \citep{kingma2014adam}. Specifically, instead of maintaining the average of the squared gradients $V_t$, AdaMax keeps track of the maximum absolute value of the past gradients \citep{kingma2014adam}.
\begin{align}
    u_t &= \max(\beta_2 \cdot u_{t-1}, |g_t|) \\ 
    \hat{m_t} &= \frac{m_t}{1 - \beta_1^t} \\
    \theta_{t+1} &= \theta_t - \alpha \cdot \frac{\hat{m_t}}{u_t}
\end{align}
Here, the second moment estimate $\hat{V_t}$ is replaced with $u_t$. This is done to make the algorithm more robust, as $u_1, u_2, \dots, u_t$ are influenced by fewer gradients and thus there is less noise. This is especially the case for when the gradients are large and can become numerically unstable \citep{kingma2014adam}. Note that no bias correction is also needed for $u_t$.

\textbf{NAdam}: Another variant of Adam is Nesterov Adam (NAdam), which extends Adam with the NAG method seen in \cref{sec:momentum} \citep{dozat2016nadam}. NAdam uses the NAG update by performing the \textit{Nesterov} trick - in which the previous first moment is replaced with the currently calculated first moment similar to NAG \citep{nesterov1983method,dozat2016nadam}. Condensing the Adam update equations from \cref{eq:m_t} to \cref{eq:adam_update} in \cref{eq:nadam_normal}, the following update is changed from 
\begin{align}
    \theta_{t+1} = \theta_t - \frac{\alpha}{{\hat{V_t}} + \epsilon} (\beta_1 \mathbf{\hat{m}_{t-1}} + \frac{(1 - \beta_1)g_t}{1 - (\beta_1)^t}) \label{eq:nadam_normal}
\end{align}
to 
\begin{align}
    \theta_{t+1} = \theta_t - \frac{\alpha}{{\hat{V_t}} + \epsilon} (\beta_1 \mathbf{\hat{m}_{t}} + \frac{(1 - \beta_1)g_t}{1 - (\beta_1)^t})
\end{align}
Like NAG, NAdam uses the future position of $\theta$ to calculate the gradient and incorporate more information in the update. NAdam has been shown to yield sizeable improvements in performance in comparison to Adam on MNIST and Word2Vec tasks \citep{dozat2016nadam}.

\textbf{AdamW}: Decoupled weight decay regularisation, or AdamW, is a further variant of Adam \citep{loshchilov2017decoupled}. Normally, applying L2 regularisation in Adam involves appending the regularisation term to the gradient update.
\begin{align}
    g_t &= \nabla_{\theta} F(\theta_t) + \lambda \theta_{t-1}
\end{align}
However, upon rescaling, the effect of the regularisation is not properly accounted for and large $g_t$ do not get regularised as much as they should \citep{loshchilov2017decoupled}. To address this, AdamW decouples the weight decay and applies it alongside the update.
\begin{align}
    \theta_{t+1} = \theta_t - \alpha \left( \frac{\hat{m_t}}{\hat{V_t} + \epsilon} + \lambda \theta_t \right)
\end{align}
This is because in adaptive gradient methods, L2 regularisation in the gradient update is \textit{not} the same as weight decay - unlike SGD \citep{loshchilov2017decoupled}. Adam is not effective with L2 regularisation, and so AdamW substantially benefits from this decoupling, improving in performance on CIFAR10 and ImageNet-32 tasks \citep{loshchilov2017decoupled}. AdamW is widely used in practice and is also a current SOTA optimisation method.

\textbf{AMSGrad}: A particularly well-known issue with Adam is it's lack of convergence guarantees. One limitation is that in certain scenarios, Adam aggressively increases the learning rate - even when the algorithm is close to the optimum \citep{reddi2019convergence}. This is because the second moment term $V_t$ can grow indefinitely, and there is a very high dependence on $\beta_2$. For particular $\beta_2$, highly suboptimal solutions can be reached in the case of simple convex settings \citep{reddi2019convergence}.

AMSGrad modifies this by keeping a bound on $\hat{V_t}$ by taking the maximum and using that for it's update rule \citep{reddi2019convergence}.
\begin{align}
    \hat{V_{t}} &= \max(\hat{V}_{t-1}, V_t)
\end{align}
This guarantees that the learning rate is not increased indefinitely, as each $V_t$ is guaranteed to be non decreasing and at least as large as the previous $V_t$'s \citep{reddi2019convergence}. By ensuring this, drastic and aggressive learning rate increases are prevented, and the algorithm convergences better and is more stable. However, AMSGrad is still dependent on $\beta_2$, and cannot show convergence guarantees on any arbitrary $\beta_2$. Some works have demonstrated that problem dependent tuning of $\beta_2$ can lead to convergence, but this still requires manual tuning.

\textbf{ADOPT}: To address the dependency of convergence on $\beta_2$, ADaptive Gradient Method with the OPTimal Convergence Rate (ADOPT) was most recently proposed. ADOPT shows that by modifying the update rules of Adam, convergence can be attained for any $\beta_2 \in [0, 1]$. The non-convergence of Adam can be attributed to the $V_t$ needing to be conditionally independent of the current gradient $g_t$. In a normal Adam update as in \cref{eq:adam_update}, the conditional independence criteria is not satisfied. This is because $\hat{V}_t$ contains information about $g_t$, and so convergence breaks. ADOPT fixes this by modifying the order of the update and the normalisation. Specifically, the normalisation is applied to the current gradient in a modified update rule of $m_t$.
\begin{align}
    m_t = \beta_1 m_{t-1} + (1 - \beta_1) \frac{g_t}{\hat{V}_{t-1} + \epsilon^2}
\end{align}
Here, the previous second moment, $V_{t-1}$ is used to ensure this conditional independence with $g_t$ when scaling and so convergence is guaranteed. In practice, ADOPT uses $\max(\hat{V}_{t-1}, \epsilon)$ instead of $\hat{V}_{t-1} + \epsilon^2$ for the normalisation for performance benefits. The parameter update is shortly applied after.
\begin{align}
    \theta_{t+1} = \theta_{t} - \alpha \cdot m_{t+1}
\end{align}
ADOPT outperforms Adam, AdamW, and AMSGrad on MNIST and CIFAR10 image classification tasks.
It achieves substantial improvements in LLM pretraining and finetuning tasks over Adam and it's aforementioned variants as well. 

\section{Higher-Order Methods}
First-order methods are effective for many optimisation problems for being simple, yet efficient. However, they fall short when encountering ill-conditioned or non-linear objective functions, and flat regions and saddle points. Higher-order methods address these issues by making use of additional curvature information. They find or approximate at least the second-order derivative, $\nabla_{\theta}^{2}F$. For brevity, we will refer to this as $H$, or $H_t$ if used stochastically. 

\subsection{Newton Methods} \label{sec:newton_methods}
\textbf{The Newton Method}: Newton's method is the simplest way to incorporate curvature information. It approximates $F$ by using a second-order Taylor expansion and updates $\theta$ by computing the search direction $d_t$, to which the solution is the gradient preconditioned with the inverse Hessian. We also note that $\alpha = 1$ usually, and learning rates are optional, but usually unused in the update step.
\begin{align}
    d_t &= H_t^{-1} g_t \\
    \theta_{t+1} &= \theta_t - \alpha d_t
\end{align}
This adjusts the step size for each parameter based on the curvature, which results in a key improvement over first-order methods. Newton steps make large progress in flat regions, and are more conservative in steep directions. The method has quadratic convergence given $F \in C^{2}$, and has strong local convergence properties if initialised near a critical point. Though this solves the slowness problem, it can result in moving in the wrong direction. Given a negative eigenvalue, it moves opposite to the gradient step which increases the error. Since it also moves towards optimal $\theta$ along directions with positive eigenvalue, it becomes attracted to saddle points.
Newton's method is also expensive, required $O(N^2)$ memory to store the Hessian and $O(N^3)$ time to compute the inverse Hessian.

\textbf{Saddle-Free Newton}: To address the saddle point concern, the saddle-free newton (SFN) approach was proposed. The SFN applies the $|\cdot|$ (abs) function onto the Hessian, where the absolute value of each eigenvalue is taken.
\begin{align}
    |H_t| &= V_{t} |\Sigma|^{-1} V_{t}^T \\
    \theta_{t+1} &= \theta_t - |H_t|^{-1} g_t
\end{align}
This allows us to escape saddle points and still keep the advantage of taking large steps in low curvature regions. However, this solution is still infeasible. It has the same memory limitations as the Newton method, and adds extra time given we need to reconstruct the Hessian by applying the abs function. Solutions to this issue involve using subspace optimisation methods, which solves the optimisation problem in a lower dimensional subspace. We will look at these in \Cref{sec:subspace_opt}.

\subsection{Quasi-Newton Methods}
\textbf{BFGS}: The computational infeasibility of Newton methods is tackled with Quasi-Newton methods. These aim to approximate the Hessian instead of computing it directly. The Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm generates a sequence of matrices to estimate the Hessian. It iteratively refines the estimate using gradient information and rank-two updates at each iteration. The estimate is always positive-definite. Through experimental testing and analysis, these estimates are quite good. BFGS also has effective self-correcting properties - if an estimate become poor at one iteration, within the next few it will correct itself \citep{NoceWrig06}. BFGS reduces the complexity to $O(N^2)$ for computation time.  

A common variant of BFGS, Limited BFGS (L-BFGS), reduces the memory requirement by storing $k$ vectors instead of retaining the full $n\times n$ approximations.

\textbf{SR1}: The Symmetric Rank-One (SR1) method further tackles the infeasibility problem. It approximates the inverse Hessian using the difference between a history of gradients and positions, while ensuring symmetry in the update. The key advantage of SR1 is that it uses only rank-updates, but also does not guarantee positive definiteness. This makes it beneficial in non-convex settings. Albeit at the cost of stability, it helps the algorithm capture more curvature information.

\textbf{Stochastic Trust Region Methods}: 

\subsection{Diagonal Hessian Estimation} \label{sec:diag_hessian}
\textbf{AdaHessian}: As Quasi-Newton methods aim to estimate the inverse Hessian, improvements have been made to use diagonal hessian estimates as well. AdaHessian modifies the second moment from Adam to use the squared diagonal estimate of the Hessian \citep{yao2021adahessian}.
\begin{align}
    \hat{D}_t &\approx \text{diag}(H_t) \\
    V_t &= \sqrt{\beta_2 V_{t-1} + (1 - \beta_2) \hat{D}_t\hat{D}_t}
\end{align}
This brings down the space complexity to $O(D)$, and makes it more feasible to use incorporate second order information \citep{yao2021adahessian}. Though, as a trade-off, an extra backward pass is required per iteration. AdaHessian performs well in image classification, NLP, and recommender system tasks \citep{yao2021adahessian}.

\textbf{Sophia} : A more effective approach to using diagonal estimates was proposed by Second-order Clipped Stochastic Optimisation (Sophia). Sophia uses a diagonal estimate of the Hessian as a preconditioner to the first moment in it's update alongside a clipping mechanism. This was motivated by heterogenous curvature in deep learning models, in which traditional optimisers struggle with. 

Sophia first computes the diagonal estimate of the Hessian, $\bar{D}_t$, every $k$ iterations using either the Hutchinson or Gauss-Newton-Bartlett estimation methods via an estimator $E$. 
\begin{align}
    \hat{D}_t &= E(\theta_t) \quad E \in \{\text{Hutchinson, Gauss-Newton-Bartlett} \} \\
    \bar{D}_t &= 
    \begin{cases}
        \beta_2 \bar{D}_{t-k} + (1 - \beta_2) \hat{D}_t & \text{if } t \mod k = 1, \\
        \bar{D}_{t-1} & \text{otherwise}
    \end{cases}
\end{align}
This is then put into a clipping function that controls the worst case size update, in which the update rule is
\begin{align}
    \theta_{t+1} = \theta_t - \alpha \cdot \text{clip}(\frac{m_t}{\max(\gamma \cdot \bar{D}_t, \epsilon)}, \rho)
\end{align}
where $\text{clip}(z, p) = \max(\min(z, p), -p)$ and $\gamma$, $\rho$ are hyper-parameters that function as a scaling factor and a clipping threshold respectively. As mentioned in \cref{sec:newton_methods}, incorporating second order information can result in moving in the wrong direction. To counter this, Sophia considers only the positive entries of $\bar{D}_t$ and clips each coordinate to a maximum step size of $p$ (where $p = 1$ usually). If any entry of $\bar{D}_t$ is negative, Sophia falls back to momentum signSGD - in which the update is scaled by the sign of the gradient for each component, ignoring the magnitude. This ensures that in the worst case, the update is controlled with a size of $p$, improving stability. On language modelling tasks, Sophia converges quicker and achieves better performance in comparison to AdamW.

\subsection{HvP-based Methods}
\textbf{Hessian-Free Optimisation}: Incorporating second order information can be done without explicitly computing the Hessian or it's approximations. Hessian-free optimisation (HF) aims to do this by directly computing \textit{Hessian-vector products} (HvP). Similar to Newton's method, a search direction is computed by solving the following linear system, where $\alpha_t$ is a step size computed to guarantee sufficient decrease.
\begin{align}
    H_t d_t &= - g_t \\ \label{hf:lin_sys}
    \theta_{t+1} &= \theta_t - \alpha_t d_t
\end{align}
The system is solved with the conjugate-gradient solver (CG), an iterative method that solves linear systems. This computes the HvP without using the Hessian, which reduces the space to $O(N)$ and results in $O(KN)$ time, where $K << N$ controls the number of iterations used for CG. However, the CG solver can be very unstable when $H_t$ isn't positive definite. Thus, it breaks down when dealing with noise and stochastically. Ill-conditioned and non-convex problems also add to these issues.

\textbf{CurveBall}: Combining fast HvP's and curvature information with the heavy-ball framework was introduced with CurveBall, hence it's name. CurveBall uses a quadratic approximation around $F$ like the Newton method, and solves the optimisation problem by solving the same linear system as in \cref{hf:lin_sys}. Instead of using CG or matrix inversion, it uses GD to optimise on $d_t$ and finds $\Delta d_t$. 
\begin{align}
\Delta d_{t+1} &= H_t d_t + g_t \\
z_{t+1} &= \beta z_{t} - \alpha_2 \Delta d_{t+1} \\
\theta_{t+1} &\leftarrow \theta_t + \alpha_1 z_{t+1}
\end{align}
The advantage to this is that there is no restriction on $H$. The algorithm can work in non-convex and ill-conditioned problems too, as demonstrated by it's performance on Rosenbrock and the Rahimi-Recht function. The $\Delta d_{t}$ variable keeps track of how $H_t$ and $g_t$ change as $\theta$ evolves, which incorporates the curvature. To amortize cost, it interleaves updates between $\theta$ and $\Delta d_{t+1}$. With the use of fast HvP's through forward-mode (FMAD) and backward-mode automatic differentiation (RMAD), it requires only two passes of back-propagation. This makes it highly efficient and scalable, in which it demonstrates better performance on MNIST and CIFAR10 classification tasks.  

\subsection{Subspace Optimisation Methods}\label{sec:subspace_opt}

\textbf{Krylov Subspace Descent}: Approximate solutions to optimisation problems in low dimensional subspaces are also a popular strategy to involve curvature information. Krylov Subspace Descent (KSD) constructs a basis of vectors $\mathcal{K}_m$ and then finds a search direction in $\mathcal{K}_m$ using BFGS. The Krylov basis is constructed with diagonal Hessian estimate preconditioning.
\begin{align}
    \mathcal{K}_m = \{(D^{-1}H)^{k}D^{-1}g | \quad 0 \leq k \leq m \}
\end{align}
During optimisation, this subspace is converted into a new non-orthogonal subspace $\hat{\mathcal{K}_m}$, where BFGS is run to find to ${\hat{K}v}$ as the search direction, where $\hat{K} \in \hat{\mathcal{K}_m}$. KSD exhibits lower training error and faster running time in comparison to HF on classification tasks such as MNIST and CURVES. It also has no assumptions on $H$, unlike HF which has stability issues if $H$ is not positive semidefinite. We note that the SFN method was initially done with KSD. 

\section{Derivative-Free Optimisation}
For some problems, there may be conditions on $F$ such that $F \notin C^1$, or $g$ is intractable to calculate. Whilst there exist approximations for $g$, derivative-free optimisation aims to find optimal $\theta$ without it.

\textbf{Coordinate Descent}: Coordinate Descent (CD) aims to perform one dimensional search across along each axis direction of $\theta$, till all directions are chosen properly to find optimal $\theta$. Naively, a set of bases $E = \{e_1, ..., e_D \}$ are chosen where $D = \dim(\theta)$. The parameter space is broken down into individual dimensions or coordinates, and optimization proceeds along each direction sequentially. As such, the update at the $j$th dimension is given by
\begin{align}
\theta^{t+1}_j = \arg\min_{\theta_j \in \mathbb{R}} F(\theta^{t+1}_1, \dots, \theta^{t+1}_{j-1}, \theta_j, \theta^t_{j+1}, \dots, \theta^t_D).
\end{align}
This guarantees convergence, as $F$ is set to decrease or stay the same at each iteration, and the convergence of CD is similar to that of GD. The main difference to GD is that each update is always axis aligned, whereas the $g$ in GD may not be aligned with any $e \in E$. Given the algorithm focuses on one dimension at a time, the update is simple, even for complex problems. It is useful for in settings where there is low $D$ or sparse structures. While highly impractical for deep learning settings, as $D$ is high and we do not know of sparsity structures, setting a coordinate basis may be beneficial to accelerate convergence. In optimisers such as Adam that are adaptive to each parameter, we see similarities as both focus on updating each dimension independently, albeit the way they do so are different. 


\section{Meta-Learning Discovery}
A new way to solve optimisation problems is to consider abstracting away the problem. Meta-learning discovery aim to formulate algorithm search such that optimisation methods are found as a result.

\textbf{Lion}: Evo\textbf{L}ved S\textbf{i}gn M\textbf{o}me\textbf{n}tum (Lion) was discovered via a meta-learning approach. It uses momentum and the sign operation to update parameters. The update is as follows:
\begin{align}
    \text{update} &= \text{sign}(\beta_1 m_{t-1} + (1 - \beta_1)g_t) \\ 
    \theta_{t+1} &= \theta_t - \alpha \cdot \text{update} \\ 
    m_t &= \beta_2 m_{t-1} + (1 - \beta_2)g_t
\end{align}
Unlike adaptive methods, there is no second moment $V_t$, and so no normalisation on the first moment is performed. Thus, Lion contributes a fixed size update to $\theta$ at each iteration, only scaling by the output of the $\text{sign}$ function. This is similar to signSGD, mentioned in \cref{sec:diag_hessian} under Sophia. Lion is advantageous given that it is simple and only tracks the momentum. This halves the memory requirement, making it more efficient. Lion gains up to a 2.3x speedup on AdamW. It outperforms AdamW on image classification, vision-language contrastive learning, diffusion modelling and language modelling and pre-training tasks. However, we note that this is the case only on transformer models that Lion is very effective. Further evaluations proving Lion's effectiveness on non-transformer based models are needed to confirm it's effectiveness.




